#!/usr/bin/env python3
"""
LLM Answer Quality Analysis Across Models

This script analyzes the quality of answers generated by three different LLM models:
- Phi3_mini_no_rephrasing
- Llama3_8B_instruct  
- Mistral_7B_instruct

We compare:
1. Individual model accuracy rates
2. Pair-wise model agreement and accuracy
3. Majority voting accuracy across all three models
4. Answer diversity and quality metrics
"""

import json
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter, defaultdict
import re
from typing import Dict, List, Tuple, Any
import warnings
warnings.filterwarnings('ignore')

# Set up plotting style
plt.style.use('default')
sns.set_palette("husl")
plt.rcParams['figure.figsize'] = (12, 8)
plt.rcParams['font.size'] = 10

def extract_numerical_answer(text):
    """Extract numerical answer from model output text."""
    if not text:
        return None
    
    # If text is already a numerical string (like '18.0' or '18'), convert directly
    if isinstance(text, str):
        # Remove any trailing .0 and convert to int
        text = text.rstrip('.0')
        if text.isdigit():
            return int(text)
    
    # Look for patterns like $18, 18, or "18" in raw text
    patterns = [
        r'\$\s*(\d+)',  # $18, $ 18
        r'\b(\d+)\s*\$',  # 18$, 18 $
        r'\b(\d+)\b',  # just 18
        r'"(\d+)"',  # "18"
        r'answer is (\d+)',  # answer is 18
        r'answer: (\d+)',  # answer: 18
        r'(\d+)\s*\$',  # 18 $ (space between number and $)
        r'\$(\d+)',  # $18 (no space)
    ]
    
    for pattern in patterns:
        match = re.search(pattern, text, re.IGNORECASE)
        if match:
            return int(match.group(1))
    
    return None

def is_answer_correct(predicted_answer, ground_truth):
    """Check if predicted answer matches ground truth."""
    if predicted_answer is None or ground_truth is None:
        return False
    
    # Convert both to integers for comparison
    try:
        predicted_int = int(predicted_answer) if isinstance(predicted_answer, str) else predicted_answer
        ground_truth_int = int(ground_truth) if isinstance(ground_truth, str) else ground_truth
        return predicted_int == ground_truth_int
    except (ValueError, TypeError):
        return False

def analyze_individual_models(results, models):
    """Analyze individual model performance."""
    print("="*50)
    print("INDIVIDUAL MODEL ANALYSIS")
    print("="*50)
    
    model_performance = {}
    
    for model_name in models:
        print(f"\nAnalyzing {model_name}")
        print("-" * 40)
        
        model_results = results[model_name]
        correct_answers = 0
        total_questions = len(model_results)
        
        # Store detailed results for each question
        question_results = []
        
        for question_idx in range(100):
            question_str = str(question_idx)
            if question_str not in model_results:
                continue
                
            question_data = model_results[question_str]
            most_frequent_answer = question_data['most_frequent_answer']
            
            # Extract numerical answer
            predicted_answer = extract_numerical_answer(most_frequent_answer)
            
            # Get ground truth from the answer file
            ground_truth = question_data.get('golden_answer')
            if ground_truth is None:
                print(f"Warning: No golden answer found for question {question_idx}")
                continue
            
            # Check if correct
            is_correct = is_answer_correct(predicted_answer, ground_truth)
            if is_correct:
                correct_answers += 1
            
            question_results.append({
                'question_idx': question_idx,
                'predicted_answer': predicted_answer,
                'ground_truth': ground_truth,
                'is_correct': is_correct,
                'most_frequent_answer': most_frequent_answer,
                'total_rollouts': question_data['total_rollouts'],
                'unique_answers': question_data['unique_answers']
            })
        
        accuracy = correct_answers / total_questions if total_questions > 0 else 0
        
        model_performance[model_name] = {
            'accuracy': accuracy,
            'correct_answers': correct_answers,
            'total_questions': total_questions,
            'question_results': question_results
        }
        
        print(f"Total Questions: {total_questions}")
        print(f"Correct Answers: {correct_answers}")
        print(f"Accuracy: {accuracy:.2%}")
        
        # Show some example answers
        print(f"\nSample answers:")
        for i, result in enumerate(question_results[:3]):
            print(f"Q{result['question_idx']}: Predicted={result['predicted_answer']}, "
                  f"Ground Truth={result['ground_truth']}, Correct={result['is_correct']}")
    
    return model_performance

def analyze_pairwise_models(results, models):
    """Analyze pair-wise model agreement and accuracy."""
    print("\n" + "="*60)
    print("PAIR-WISE MODEL ANALYSIS")
    print("="*60)
    
    pairwise_analysis = {}
    
    for i, model1 in enumerate(models):
        for j, model2 in enumerate(models[i+1:], i+1):
            pair_name = f"{model1} + {model2}"
            print(f"\nAnalyzing pair: {pair_name}")
            print("-" * 50)
            
            agreement_count = 0
            correct_when_agree = 0
            total_questions = 0
            
            pair_results = []
            
            for question_idx in range(100):
                question_str = str(question_idx)
                if (question_str not in results[model1] or 
                    question_str not in results[model2]):
                    continue
                
                # Get answers from both models
                answer1 = extract_numerical_answer(results[model1][question_str]['most_frequent_answer'])
                answer2 = extract_numerical_answer(results[model2][question_str]['most_frequent_answer'])
                
                # Check if they agree
                models_agree = (answer1 == answer2)
                
                # Get ground truth from either model's data
                ground_truth = results[model1][question_str].get('golden_answer')
                if ground_truth is None:
                    ground_truth = results[model2][question_str].get('golden_answer')
                
                if ground_truth is None:
                    print(f"Warning: No golden answer found for question {question_idx}")
                    continue
                
                if models_agree:
                    agreement_count += 1
                    # Check if agreed answer is correct
                    is_correct = is_answer_correct(answer1, ground_truth)
                    if is_correct:
                        correct_when_agree += 1
                
                total_questions += 1
                
                pair_results.append({
                    'question_idx': question_idx,
                    'model1_answer': answer1,
                    'model2_answer': answer2,
                    'models_agree': models_agree,
                    'agreed_answer': answer1 if models_agree else None,
                    'ground_truth': ground_truth,
                    'is_correct_when_agree': is_answer_correct(answer1, ground_truth) if models_agree else None
                })
            
            agreement_rate = agreement_count / total_questions if total_questions > 0 else 0
            accuracy_when_agree = correct_when_agree / agreement_count if agreement_count > 0 else 0
            
            pairwise_analysis[pair_name] = {
                'agreement_rate': agreement_rate,
                'accuracy_when_agree': accuracy_when_agree,
                'agreement_count': agreement_count,
                'correct_when_agree': correct_when_agree,
                'total_questions': total_questions,
                'pair_results': pair_results
            }
            
            print(f"Total Questions: {total_questions}")
            print(f"Agreement Count: {agreement_count}")
            print(f"Agreement Rate: {agreement_rate:.2%}")
            print(f"Accuracy When Agree: {accuracy_when_agree:.2%}")
            
            # Show some examples of agreement/disagreement
            print(f"\nSample pair results:")
            for k, result in enumerate(pair_results[:3]):
                print(f"Q{result['question_idx']}: {model1}={result['model1_answer']}, "
                      f"{model2}={result['model2_answer']}, Agree={result['models_agree']}")
    
    return pairwise_analysis

def analyze_majority_voting(results, models):
    """Analyze majority voting across all three models."""
    print(f"\n{'='*70}")
    print(f"MAJORITY VOTING ANALYSIS (ALL THREE MODELS)")
    print(f"{'='*70}")
    
    majority_results = []
    correct_majority = 0
    total_questions = 0
    
    for question_idx in range(100):
        question_str = str(question_idx)
        # Check if all models have results for this question
        if not all(question_str in results[model] for model in models):
            continue
        
        # Get answers from all three models
        model_answers = {}
        for model in models:
            answer = extract_numerical_answer(results[model][question_str]['most_frequent_answer'])
            model_answers[model] = answer
        
        # Find majority answer
        answer_counts = Counter([ans for ans in model_answers.values() if ans is not None])
        
        if answer_counts:
            majority_answer = answer_counts.most_common(1)[0][0]
            majority_count = answer_counts[majority_answer]
            
            # Check if majority answer is correct
            # Get ground truth from any model's data
            ground_truth = None
            for model in models:
                if question_str in results[model]:
                    ground_truth = results[model][question_str].get('golden_answer')
                    if ground_truth is not None:
                        break
            
            if ground_truth is None:
                print(f"Warning: No golden answer found for question {question_idx}")
                continue
            
            is_correct = is_answer_correct(majority_answer, ground_truth)
            
            if is_correct:
                correct_majority += 1
            
            total_questions += 1
            
            majority_results.append({
                'question_idx': question_idx,
                'model_answers': model_answers,
                'majority_answer': majority_answer,
                'majority_count': majority_count,
                'ground_truth': ground_truth,
                'is_correct': is_correct,
                'answer_distribution': dict(answer_counts)
            })
    
    majority_accuracy = correct_majority / total_questions if total_questions > 0 else 0
    
    print(f"Total Questions: {total_questions}")
    print(f"Correct Majority Answers: {correct_majority}")
    print(f"Majority Voting Accuracy: {majority_accuracy:.2%}")
    
    # Show some examples
    print(f"\nSample majority voting results:")
    for i, result in enumerate(majority_results[:3]):
        print(f"Q{result['question_idx']}: ")
        for model, answer in result['model_answers'].items():
            print(f"  {model}: {answer}")
        print(f"  Majority: {result['majority_answer']} (Count: {result['majority_count']})")
        print(f"  Ground Truth: {result['ground_truth']}, Correct: {result['is_correct']}")
        print()
    
    return majority_results, majority_accuracy, correct_majority, total_questions

def create_visualizations(summary_data, model_performance, pairwise_analysis):
    """Create comprehensive visualizations."""
    print("\nCreating visualizations...")
    
    # Create summary DataFrame
    summary_df = pd.DataFrame(summary_data)
    
    # Create visualizations
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))
    
    # 1. Individual Model Accuracy
    individual_models = [data['Model'] for data in summary_data if data['Type'] == 'Individual']
    individual_accuracies = [data['Accuracy'] for data in summary_data if data['Type'] == 'Individual']
    
    ax1.bar(individual_models, individual_accuracies, color=['#FF6B6B', '#4ECDC4', '#45B7D1'])
    ax1.set_title('Individual Model Accuracy', fontsize=14, fontweight='bold')
    ax1.set_ylabel('Accuracy')
    ax1.set_ylim(0, 1)
    for i, v in enumerate(individual_accuracies):
        ax1.text(i, v + 0.01, f'{v:.1%}', ha='center', va='bottom')
    
    # 2. Pairwise Agreement Rates
    pairwise_models = [data['Model'] for data in summary_data if data['Type'] == 'Pairwise']
    pairwise_agreements = [data['Accuracy'] for data in summary_data if data['Type'] == 'Pairwise']
    
    ax2.bar(range(len(pairwise_models)), pairwise_agreements, color=['#96CEB4', '#FFEAA7', '#DDA0DD'])
    ax2.set_title('Pairwise Model Agreement Accuracy', fontsize=14, fontweight='bold')
    ax2.set_ylabel('Accuracy When Models Agree')
    ax2.set_xticks(range(len(pairwise_models)))
    ax2.set_xticklabels(pairwise_models, rotation=45, ha='right')
    ax2.set_ylim(0, 1)
    for i, v in enumerate(pairwise_agreements):
        ax2.text(i, v + 0.01, f'{v:.1%}', ha='center', va='bottom')
    
    # 3. Overall Performance Comparison
    all_models = [data['Model'] for data in summary_data]
    all_accuracies = [data['Accuracy'] for data in summary_data]
    colors = ['#FF6B6B' if 'Individual' in data['Type'] else '#4ECDC4' if 'Pairwise' in data['Type'] else '#45B7D1' 
              for data in summary_data]
    
    ax3.bar(range(len(all_models)), all_accuracies, color=colors)
    ax3.set_title('Overall Performance Comparison', fontsize=14, fontweight='bold')
    ax3.set_ylabel('Accuracy')
    ax3.set_xticks(range(len(all_models)))
    ax3.set_xticklabels(all_models, rotation=45, ha='right')
    ax3.set_ylim(0, 1)
    for i, v in enumerate(all_accuracies):
        ax3.text(i, v + 0.01, f'{v:.1%}', ha='center', va='bottom')
    
    # 4. Answer Distribution Example (for one question)
    if model_performance:
        first_model = list(model_performance.keys())[0]
        if model_performance[first_model]['question_results']:
            sample_result = model_performance[first_model]['question_results'][0]
            answer_counts = Counter([r['predicted_answer'] for r in model_performance[first_model]['question_results'] if r['predicted_answer'] is not None])
            
            if answer_counts:
                answers = list(answer_counts.keys())[:5]  # Top 5 answers
                counts = [answer_counts[ans] for ans in answers]
                
                ax4.pie(counts, labels=answers, autopct='%1.1f%%', startangle=90)
                ax4.set_title(f'Answer Distribution for {first_model}', fontsize=14, fontweight='bold')
    
    plt.tight_layout()
    plt.savefig('llm_analysis_visualizations.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    return summary_df

def detailed_analysis(model_performance, pairwise_analysis, majority_results, majority_accuracy, correct_majority):
    """Perform detailed analysis and provide insights."""
    print("\n" + "="*80)
    print("DETAILED ANALYSIS AND INSIGHTS")
    print("="*80)
    
    models = list(model_performance.keys())
    
    # Model confidence analysis
    print("\n1. MODEL CONFIDENCE ANALYSIS:")
    for model_name in models:
        model_results = model_performance[model_name]['question_results']
        avg_rollouts = np.mean([data['total_rollouts'] for data in model_results])
        avg_unique_answers = np.mean([data['unique_answers'] for data in model_results])
        
        print(f"\n{model_name}:")
        print(f"  Average rollouts per question: {avg_rollouts:.1f}")
        print(f"  Average unique answers per question: {avg_unique_answers:.1f}")
        print(f"  Answer diversity ratio: {avg_unique_answers/avg_rollouts:.2%}")
    
    # Agreement patterns
    print("\n\n2. MODEL AGREEMENT PATTERNS:")
    for pair_name, perf in pairwise_analysis.items():
        print(f"\n{pair_name}:")
        print(f"  Agreement rate: {perf['agreement_rate']:.1%}")
        print(f"  Accuracy when agreeing: {perf['accuracy_when_agree']:.1%}")
        
        # Calculate disagreement accuracy
        disagreement_results = [r for r in perf['pair_results'] if not r['models_agree']]
        if disagreement_results:
            disagreement_correct = sum(1 for r in disagreement_results 
                                      if is_answer_correct(r['model1_answer'], r['ground_truth']) or 
                                         is_answer_correct(r['model2_answer'], r['ground_truth']))
            disagreement_accuracy = disagreement_correct / len(disagreement_results)
            print(f"  Disagreement accuracy (either model correct): {disagreement_accuracy:.1%}")
    
    # Majority voting insights
    print("\n\n3. MAJORITY VOTING INSIGHTS:")
    if majority_results:
        unanimous_agreement = sum(1 for r in majority_results 
                                 if len(set(r['answer_distribution'].keys())) == 1)
        split_agreement = len(majority_results) - unanimous_agreement
        
        print(f"  Unanimous agreement cases: {unanimous_agreement}")
        print(f"  Split decision cases: {split_agreement}")
        
        # Analyze unanimous vs split cases
        unanimous_correct = sum(1 for r in majority_results 
                               if len(set(r['answer_distribution'].keys())) == 1 and r['is_correct'])
        unanimous_accuracy = unanimous_correct / unanimous_agreement if unanimous_agreement > 0 else 0
        
        split_correct = sum(1 for r in majority_results 
                            if len(set(r['answer_distribution'].keys())) > 1 and r['is_correct'])
        split_accuracy = split_correct / split_agreement if split_agreement > 0 else 0
        
        print(f"  Unanimous agreement accuracy: {unanimous_accuracy:.1%}")
        print(f"  Split decision accuracy: {split_accuracy:.1%}")
    
    # Performance ranking
    print("\n\n4. PERFORMANCE RANKING:")
    summary_data = []
    
    # Individual model performance
    for model_name, perf in model_performance.items():
        summary_data.append({
            'Model': model_name,
            'Type': 'Individual',
            'Accuracy': perf['accuracy'],
            'Correct': perf['correct_answers'],
            'Total': perf['total_questions']
        })
    
    # Pairwise performance
    for pair_name, perf in pairwise_analysis.items():
        summary_data.append({
            'Model': pair_name,
            'Type': 'Pairwise',
            'Accuracy': perf['accuracy_when_agree'],
            'Correct': perf['correct_when_agree'],
            'Total': perf['agreement_count']
        })
    
    # Majority voting
    summary_data.append({
        'Model': 'Majority Voting (All 3)',
        'Type': 'Majority',
        'Accuracy': majority_accuracy,
        'Correct': correct_majority,
        'Total': len(majority_results) if majority_results else 0
    })
    
    performance_ranking = sorted(summary_data, key=lambda x: x['Accuracy'], reverse=True)
    for i, (rank, data) in enumerate(enumerate(performance_ranking, 1)):
        print(f"  {rank}. {data['Model']}: {data['Accuracy']:.1%}")
    
    # Recommendations
    print("\n\n5. RECOMMENDATIONS:")
    best_individual = max([d for d in summary_data if d['Type'] == 'Individual'], key=lambda x: x['Accuracy'])
    best_pair = max([d for d in summary_data if d['Type'] == 'Pairwise'], key=lambda x: x['Accuracy'])
    best_majority = [d for d in summary_data if d['Type'] == 'Majority'][0]
    
    print(f"  Best individual model: {best_individual['Model']} ({best_individual['Accuracy']:.1%})")
    print(f"  Best pairwise combination: {best_pair['Model']} ({best_pair['Accuracy']:.1%})")
    print(f"  Majority voting performance: {best_majority['Accuracy']:.1%}")
    
    if best_majority['Accuracy'] > best_individual['Accuracy']:
        print("  → Majority voting improves performance over individual models!")
    else:
        print("  → Individual model performs better than majority voting.")
    
    print("\n" + "="*80)
    
    return summary_data

def export_results(model_performance, pairwise_analysis, majority_results, summary_data):
    """Export detailed results to CSV files."""
    print("Exporting detailed results...")
    
    models = list(model_performance.keys())
    
    # Individual model results
    for model_name in models:
        df = pd.DataFrame(model_performance[model_name]['question_results'])
        filename = f"{model_name.replace(' ', '_').replace('-', '_')}_detailed_results.csv"
        df.to_csv(filename, index=False)
        print(f"  Exported {filename}")
    
    # Pairwise results
    for pair_name in pairwise_analysis.keys():
        df = pd.DataFrame(pairwise_analysis[pair_name]['pair_results'])
        filename = f"{pair_name.replace(' ', '_').replace('+', '_plus_')}_results.csv"
        df.to_csv(filename, index=False)
        print(f"  Exported {filename}")
    
    # Majority voting results
    if majority_results:
        df = pd.DataFrame(majority_results)
        df.to_csv('majority_voting_results.csv', index=False)
        print(f"  Exported majority_voting_results.csv")
    
    # Summary results
    summary_df = pd.DataFrame(summary_data)
    summary_df.to_csv('performance_summary.csv', index=False)
    print(f"  Exported performance_summary.csv")
    
    print("\nAll results exported successfully!")

def main():
    """Main function to run the complete analysis."""
    print("🚀 Starting LLM Answer Quality Analysis...")
    
    # Load the extracted LLM answers
    try:
        with open('llm_answers_discriminator_0_99.json', 'r') as f:
            data = json.load(f)
        print(f"✅ Data loaded successfully!")
        print(f"LLM Models: {data['metadata']['llm_generators']}")
        print(f"Question Range: {data['metadata']['question_range']}")
        print(f"Total Questions: {data['metadata']['total_questions']}")
    except FileNotFoundError:
        print("❌ Error: llm_answers_discriminator_0_99.json not found!")
        print("Please run the extraction script first.")
        return
    
    # Extract results
    results = data['results']
    models = list(results.keys())
    print(f"\nAvailable models: {models}")
    
    # Run analysis
    print("\n" + "="*80)
    print("RUNNING COMPREHENSIVE ANALYSIS")
    print("="*80)
    
    # 1. Individual model analysis
    model_performance = analyze_individual_models(results, models)
    
    # 2. Pairwise analysis
    pairwise_analysis = analyze_pairwise_models(results, models)
    
    # 3. Majority voting analysis
    majority_results, majority_accuracy, correct_majority, total_questions = analyze_majority_voting(results, models)
    
    # 4. Create visualizations
    summary_data = []
    
    # Individual model performance
    for model_name, perf in model_performance.items():
        summary_data.append({
            'Model': model_name,
            'Type': 'Individual',
            'Accuracy': perf['accuracy'],
            'Correct': perf['correct_answers'],
            'Total': perf['total_questions']
        })
    
    # Pairwise performance
    for pair_name, perf in pairwise_analysis.items():
        summary_data.append({
            'Model': pair_name,
            'Type': 'Pairwise',
            'Accuracy': perf['accuracy_when_agree'],
            'Correct': perf['correct_when_agree'],
            'Total': perf['agreement_count']
        })
    
    # Majority voting
    summary_data.append({
        'Model': 'Majority Voting (All 3)',
        'Type': 'Majority',
        'Accuracy': majority_accuracy,
        'Correct': correct_majority,
        'Total': total_questions
    })
    
    summary_df = create_visualizations(summary_data, model_performance, pairwise_analysis)
    
    # 5. Detailed analysis
    summary_data = detailed_analysis(model_performance, pairwise_analysis, majority_results, majority_accuracy, correct_majority)
    
    # 6. Export results
    export_results(model_performance, pairwise_analysis, majority_results, summary_data)
    
    print("\n" + "="*80)
    print("ANALYSIS COMPLETE!")
    print("="*80)
    print("📊 Check the generated CSV files and visualization for detailed results.")
    print("🎯 The analysis shows accuracy rates, model agreement patterns, and majority voting performance.")

if __name__ == "__main__":
    main() 